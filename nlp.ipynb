{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (3.0.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing\n",
      "Zoho\n",
      "Publish\n",
      ",\n",
      "an\n",
      "all\n",
      "-\n",
      "in\n",
      "-\n",
      "one\n",
      "platform\n",
      "for\n",
      "chain\n",
      "and\n",
      "franchise\n",
      "businesses\n",
      "to\n",
      "boost\n",
      "reputation\n",
      "and\n",
      "improve\n",
      "online\n",
      "presence\n",
      "on\n",
      "top\n",
      "business\n",
      "directories\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "#understands english\n",
    "doc = nlp(\"Introducing Zoho Publish, an all-in-one platform for chain and franchise businesses to boost reputation and improve online presence on top business directories.\")\n",
    "for token in doc : \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Introducing\n",
      "Zoho\n",
      "Publish\n",
      ",\n",
      "an\n",
      "all\n",
      "-\n",
      "in\n",
      "-\n",
      "one\n",
      "platform\n",
      "for\n",
      "chain\n",
      "and\n",
      "franchise\n",
      "businesses\n",
      "to\n",
      "boost\n",
      "reputation\n",
      "and\n",
      "improve\n",
      "online\n",
      "presence\n",
      "on\n",
      "top\n",
      "business\n",
      "directories\n",
      ".\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "#understands english\n",
    "doc = nlp('''\"Introducing Zoho Publish, an all-in-one platform for chain and franchise businesses to boost reputation and improve online presence on top business directories.\"''')\n",
    "for token in doc : \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0.is_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name, Email\\n',\n",
       " 'John Doe, john.doe@example.com\\n',\n",
       " 'Jane Smith, jane.smith@example.com\\n',\n",
       " 'Emily Davis, emily.davis@example.com\\n',\n",
       " 'Michael Brown, michael.brown@example.com\\n',\n",
       " 'Sarah Wilson, sarah.wilson@example.com\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students_info.txt\") as f :\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 19.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 19.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john.doe@example.com\n",
      "jane.smith@example.com\n",
      "emily.davis@example.com\n",
      "michael.brown@example.com\n",
      "sarah.wilson@example.com\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read the file and join lines into a single string\n",
    "with open(\"students_info.txt\") as f:\n",
    "    text = f.read()  # Use read() instead of readlines() to get the entire content as a string\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Identify and print email-like tokens\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "சூப்பர் False\n",
      "ஸ்டார் False\n",
      "ரஜினிகாந்த் False\n",
      ", False\n",
      "இந்திய False\n",
      "திரைப்பட False\n",
      "உலகின் False\n",
      "பிரபலமான False\n",
      "நடிகராக False\n",
      "திகழ்கிறார் False\n",
      ". False\n",
      "இவர் False\n",
      "இதுவரை False\n",
      "170 True\n",
      "க்கும் False\n",
      "மேற்பட்ட False\n",
      "திரைப்படங்களில் False\n",
      "நடித்துள்ளார் False\n",
      ". False\n",
      "அவரது False\n",
      "திறமையான False\n",
      "நடிப்பும் False\n",
      ", False\n",
      "எளிமையான False\n",
      "தன்மையும் False\n",
      "கோடிக்கணக்கான False\n",
      "ரசிகர்களின் False\n",
      "மனதில் False\n",
      "இடம்பிடித்துள்ளது False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('ta')\n",
    "\n",
    "text = \"சூப்பர் ஸ்டார் ரஜினிகாந்த், இந்திய திரைப்பட உலகின் பிரபலமான நடிகராக திகழ்கிறார். இவர் இதுவரை 170 க்கும் மேற்பட்ட திரைப்படங்களில் நடித்துள்ளார். அவரது திறமையான நடிப்பும், எளிமையான தன்மையும் கோடிக்கணக்கான ரசிகர்களின் மனதில் இடம்பிடித்துள்ளது.\"\n",
    "doc = nlp(text)\n",
    "for token in doc :\n",
    "    print(token, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'continue', 'to', 'face', 'issues', ',', 'share', 'the', 'full', 'traceback', 'or', 'context', 'for', 'further', 'assistance', 'gimme', 'a', 'call']\n"
     ]
    }
   ],
   "source": [
    "text = \"If you continue to face issues, share the full traceback or context for further assistance gimme a call\"\n",
    "doc = nlp(text)\n",
    "tokens = [ token.text for token in doc ]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'the', 'book']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a special case to the tokenizer for \"gimme\"\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "\n",
    "# Test the tokenizer with the special case\n",
    "doc = nlp(\"gimme the book\")\n",
    "print([token.text for token in doc])  # Output: ['give', 'me', 'the', 'book']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
      "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup.\n",
      "\n",
      "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen.\n",
      "\n",
      "Here are some other facts about MS Dhoni.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the sentencizer to the pipeline\n",
    "nlp.add_pipe('sentencizer', first=True)\n",
    "\n",
    "# Sample text\n",
    "text = '''Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
    "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup. \n",
    "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen. \n",
    "Here are some other facts about MS Dhoni.'''\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the sentences\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
      "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup.\n",
      "\n",
      "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen.\n",
      "\n",
      "Here are some other facts about MS Dhoni:\n",
      "Dhoni was born on July 7, 1981 in Ranchi, Bihar.\n",
      "\n",
      "He made his international debut in 2004 against Bangladesh in Chittagong.\n",
      "\n",
      "He captained the Indian cricket team in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014.\n"
     ]
    }
   ],
   "source": [
    "text = '''Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
    "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup. \n",
    "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen. \n",
    "Here are some other facts about MS Dhoni:\n",
    "Dhoni was born on July 7, 1981 in Ranchi, Bihar. \n",
    "He made his international debut in 2004 against Bangladesh in Chittagong. \n",
    "He captained the Indian cricket team in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. '''\n",
    "doc = nlp(text)\n",
    "\n",
    "for sentence  in doc.sents :\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not adding pipe using inbuilt pipeline models\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1973aa17bf0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1973aa17d70>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x197520c0f90>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x197521842d0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1975325d010>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1974f481380>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here ADV\n",
      "are AUX\n",
      "two NUM\n",
      "short ADJ\n",
      "sentences NOUN\n",
      "about ADP\n",
      "Mahendra PROPN\n",
      "Singh PROPN\n",
      "Dhoni PROPN\n",
      "( PUNCT\n",
      "MS PROPN\n",
      "Dhoni PROPN\n",
      ") PUNCT\n",
      ", PUNCT\n",
      "an DET\n",
      "Indian ADJ\n",
      "cricketer NOUN\n",
      ": PUNCT\n",
      "\n",
      " SPACE\n",
      "MS PROPN\n",
      "Dhoni PROPN\n",
      "is AUX\n",
      "a DET\n",
      "former ADJ\n",
      "Indian ADJ\n",
      "cricketer NOUN\n",
      "who PRON\n",
      "captained VERB\n",
      "the DET\n",
      "Indian ADJ\n",
      "national ADJ\n",
      "team NOUN\n",
      "to ADP\n",
      "victory NOUN\n",
      "in ADP\n",
      "the DET\n",
      "2011 NUM\n",
      "Cricket PROPN\n",
      "World PROPN\n",
      "Cup PROPN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n",
      "Dhoni PROPN\n",
      "is AUX\n",
      "considered VERB\n",
      "one NUM\n",
      "of ADP\n",
      "the DET\n",
      "most ADV\n",
      "successful ADJ\n",
      "Indian ADJ\n",
      "captains NOUN\n",
      "and CCONJ\n",
      "most ADJ\n",
      "prolific ADJ\n",
      "wicket NOUN\n",
      "- PUNCT\n",
      "keeper NOUN\n",
      "batsmen NOUN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n",
      "Here ADV\n",
      "are AUX\n",
      "some DET\n",
      "other ADJ\n",
      "facts NOUN\n",
      "about ADP\n",
      "MS PROPN\n",
      "Dhoni PROPN\n",
      ": PUNCT\n",
      "\n",
      " SPACE\n",
      "Dhoni PROPN\n",
      "was AUX\n",
      "born VERB\n",
      "on ADP\n",
      "July PROPN\n",
      "7 NUM\n",
      ", PUNCT\n",
      "1981 NUM\n",
      "in ADP\n",
      "Ranchi PROPN\n",
      ", PUNCT\n",
      "Bihar PROPN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n",
      "He PRON\n",
      "made VERB\n",
      "his PRON\n",
      "international ADJ\n",
      "debut NOUN\n",
      "in ADP\n",
      "2004 NUM\n",
      "against ADP\n",
      "Bangladesh PROPN\n",
      "in ADP\n",
      "Chittagong PROPN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n",
      "He PRON\n",
      "captained VERB\n",
      "the DET\n",
      "Indian ADJ\n",
      "cricket NOUN\n",
      "team NOUN\n",
      "in ADP\n",
      "limited ADJ\n",
      "overs NOUN\n",
      "formats NOUN\n",
      "from ADP\n",
      "2007 NUM\n",
      "to ADP\n",
      "2017 NUM\n",
      "and CCONJ\n",
      "in ADP\n",
      "test NOUN\n",
      "cricket NOUN\n",
      "from ADP\n",
      "2008 NUM\n",
      "to ADP\n",
      "2014 NUM\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "text = '''Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
    "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup. \n",
    "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen. \n",
    "Here are some other facts about MS Dhoni:\n",
    "Dhoni was born on July 7, 1981 in Ranchi, Bihar. \n",
    "He made his international debut in 2004 against Bangladesh in Chittagong. \n",
    "He captained the Indian cricket team in limited overs formats from 2007 to 2017 and in test cricket from 2008 to 2014. '''\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc :\n",
    "    print(token , token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two CARDINAL\n",
      "Mahendra Singh Dhoni PERSON\n",
      "MS Dhoni PERSON\n",
      "Indian NORP\n",
      "MS Dhoni PERSON\n",
      "Indian NORP\n",
      "Indian NORP\n",
      "2011 DATE\n",
      "Dhoni PERSON\n",
      "one CARDINAL\n",
      "Indian NORP\n",
      "MS Dhoni PERSON\n",
      "Dhoni PERSON\n",
      "July 7, 1981 DATE\n",
      "Ranchi GPE\n",
      "Bihar GPE\n",
      "2004 DATE\n",
      "Bangladesh GPE\n",
      "Chittagong GPE\n",
      "Indian NORP\n",
      "2007 DATE\n",
      "2017 DATE\n",
      "2008 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for ent in doc.ents :\n",
    "    print(ent.text , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Here are \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " short sentences about \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mahendra Singh Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MS Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "), an \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " cricketer:<br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MS Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is a former \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " cricketer who captained the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " national team to victory in the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2011\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " Cricket World Cup. <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is considered \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of the most successful \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " captains and most prolific wicket-keeper batsmen. <br>Here are some other facts about \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MS Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ":<br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Dhoni\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was born on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    July 7, 1981\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ranchi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bihar\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". <br>He made his international debut in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2004\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " against \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bangladesh\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chittagong\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". <br>He captained the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Indian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " cricket team in limited overs formats from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2017\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and in test cricket from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2008\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " to 2014. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "44th ORDINAL\n",
      "the United States GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Assume source_nlp is a pre-trained SpaCy pipeline with an 'ner' component\n",
    "# Replace 'source_nlp' with an actual model, e.g., spacy.load(\"en_core_web_sm\")\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the NER component from the source pipeline\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "\n",
    "# Process text\n",
    "text = \"Barack Obama was the 44th President of the United States.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#steaming : getting the base word from the word removing ing\n",
    "#lemmatization finding the related words using linguistic knowleage eg eat ate\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "walking | walk\n",
      "ananthing | ananth\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer , SnowballStemmer\n",
    "stemmer = PorterStemmer()\n",
    "words = ['eating','walking','ananthing']\n",
    "\n",
    "for i in words:\n",
    "    print(i ,'|', stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "bro | brother\n",
      "I | I\n",
      "ate | eat\n",
      "the | the\n",
      "food | food\n",
      "yesterday | yesterday\n",
      "and | and\n",
      "wrote | write\n",
      "a | a\n",
      "letter | letter\n",
      "to | to\n",
      "my | my\n",
      "mom | mom\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "text = \"bro I ate the food yesterday and wrote a letter to my mom\"\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "# Check pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Get the attribute ruler\n",
    "ar = nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "# Add custom rules to the attribute ruler\n",
    "patterns = [[{\"LOWER\": \"bro\"}], [{\"LOWER\": \"bruh\"}]]\n",
    "attrs = {\"LEMMA\": \"brother\"}\n",
    "ar.add(patterns=patterns, attrs=attrs)\n",
    "\n",
    "# Process the text again to see the updated lemma\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, '|', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text to analyze\n",
    "text = \"tesla buying apple and meta\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Define a mapping for additional descriptions\n",
    "entity_descriptions = {\n",
    "    \"tesla\": \"company, agency\",\n",
    "    \"apple\": \"company, agency\",\n",
    "    \"meta\": \"company, agency\"\n",
    "}\n",
    "\n",
    "# Print detected named entities, their labels, and descriptions\n",
    "for ent in doc.ents:\n",
    "    description = entity_descriptions.get(ent.text.lower(), \"unknown\")\n",
    "    print(f\"{ent.text} | {ent.label_} | {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data =  pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Category.value_counts()\n",
    "# it as two category check toal values in each catory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = data['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message  spam\n",
       "0         ham  Go until jurong point, crazy.. Available only ...     0\n",
       "1         ham                      Ok lar... Joking wif u oni...     0\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3         ham  U dun say so early hor... U c already then say...     0\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...     0\n",
       "...       ...                                                ...   ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...     1\n",
       "5568      ham               Will ü b going to esplanade fr home?     0\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...     0\n",
       "5570      ham  The guy did some bitching but I acted like i'd...     0\n",
       "5571      ham                         Rofl. Its true to its name     0\n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(data.Message , data.spam , test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate the CountVectorizer\n",
    "v = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "x_train_transformed = v.fit_transform(x_train.values)\n",
    "\n",
    "# create matrix for each message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 58991 stored elements and shape (4457, 7705)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_transformed.toarray()[:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 980,\n",
       " 'that': 6778,\n",
       " 'fine': 2854,\n",
       " 'got': 3202,\n",
       " 'enough': 2611,\n",
       " 'bud': 1557,\n",
       " 'to': 6899,\n",
       " 'last': 4023,\n",
       " 'most': 4594,\n",
       " 'of': 4890,\n",
       " 'the': 6782,\n",
       " 'night': 4780,\n",
       " 'at': 1135,\n",
       " 'least': 4059,\n",
       " 'send': 5990,\n",
       " 'logo': 4179,\n",
       " 'ur': 7173,\n",
       " 'lover': 4230,\n",
       " 'names': 4691,\n",
       " 'joined': 3840,\n",
       " 'by': 1602,\n",
       " 'heart': 3383,\n",
       " 'txt': 7064,\n",
       " 'love': 4225,\n",
       " 'name1': 4689,\n",
       " 'name2': 4690,\n",
       " 'mobno': 4550,\n",
       " 'eg': 2549,\n",
       " 'adam': 824,\n",
       " 'eve': 2663,\n",
       " '07123456789': 26,\n",
       " '87077': 699,\n",
       " 'yahoo': 7630,\n",
       " 'pobox36504w45wq': 5258,\n",
       " 'txtno': 7070,\n",
       " 'no': 4793,\n",
       " 'ads': 846,\n",
       " '150p': 309,\n",
       " 'waiting': 7326,\n",
       " 'for': 2937,\n",
       " 'your': 7671,\n",
       " 'call': 1618,\n",
       " 'chance': 1727,\n",
       " 'win': 7482,\n",
       " '250': 373,\n",
       " 'cash': 1681,\n",
       " 'every': 2671,\n",
       " 'wk': 7518,\n",
       " 'play': 5227,\n",
       " '83370': 674,\n",
       " 'www': 7603,\n",
       " 'music': 4656,\n",
       " 'trivia': 7013,\n",
       " 'net': 4748,\n",
       " 'custcare': 2126,\n",
       " '08715705022': 133,\n",
       " '1x150p': 346,\n",
       " 'wait': 7323,\n",
       " 'still': 6459,\n",
       " 'not': 4825,\n",
       " 'all': 931,\n",
       " 'clear': 1844,\n",
       " 'were': 7425,\n",
       " 'you': 7667,\n",
       " 'sure': 6598,\n",
       " 'about': 771,\n",
       " 'me': 4407,\n",
       " 'being': 1322,\n",
       " 'sarcastic': 5886,\n",
       " 'or': 4975,\n",
       " 'why': 7465,\n",
       " 'doesn': 2389,\n",
       " 'want': 7347,\n",
       " 'live': 4149,\n",
       " 'with': 7508,\n",
       " 'us': 7188,\n",
       " 'piggy': 5202,\n",
       " 'awake': 1184,\n",
       " 'bet': 1344,\n",
       " 're': 5567,\n",
       " 'sleeping': 6203,\n",
       " 'going': 3175,\n",
       " 'lunch': 4259,\n",
       " 'now': 4839,\n",
       " 'petey': 5163,\n",
       " 'boy': 1484,\n",
       " 'whereare': 7449,\n",
       " 'friendsare': 3003,\n",
       " 'in': 3641,\n",
       " 'thekingshead': 6788,\n",
       " 'come': 1915,\n",
       " 'down': 2433,\n",
       " 'if': 3598,\n",
       " 'canlove': 1649,\n",
       " 'nic': 4774,\n",
       " 'really': 5587,\n",
       " 'sorry': 6308,\n",
       " 'lit': 4146,\n",
       " 'hair': 3300,\n",
       " 'on': 4933,\n",
       " 'fire': 2864,\n",
       " 'sparkling': 6336,\n",
       " 'shopping': 6093,\n",
       " 'breaks': 1507,\n",
       " 'from': 3017,\n",
       " '45': 507,\n",
       " 'per': 5134,\n",
       " 'person': 5150,\n",
       " '0121': 4,\n",
       " '2025050': 357,\n",
       " 'visit': 7276,\n",
       " 'shortbreaks': 6098,\n",
       " 'org': 4987,\n",
       " 'uk': 7091,\n",
       " 'hey': 3419,\n",
       " 'congrats': 1971,\n",
       " '2u2': 423,\n",
       " 'id': 3585,\n",
       " 'luv': 4263,\n",
       " 'but': 1588,\n",
       " 'ive': 3771,\n",
       " 'had': 3293,\n",
       " 'go': 3163,\n",
       " 'home': 3471,\n",
       " 'don': 2410,\n",
       " 'think': 6813,\n",
       " 'what': 7437,\n",
       " 'have': 3359,\n",
       " 'how': 3515,\n",
       " 'use': 7191,\n",
       " 'it': 3757,\n",
       " 'good': 3184,\n",
       " 'ni8': 4773,\n",
       " 'car': 1657,\n",
       " 'my': 4667,\n",
       " 'mum': 4642,\n",
       " 'lor': 4203,\n",
       " 'leh': 4072,\n",
       " 'reach': 5568,\n",
       " 'already': 942,\n",
       " 'back': 1209,\n",
       " 'amp': 968,\n",
       " 'we': 7380,\n",
       " 'packing': 5039,\n",
       " 'll': 4157,\n",
       " 'let': 4083,\n",
       " 'know': 3961,\n",
       " 'there': 6798,\n",
       " 'room': 5798,\n",
       " 'can': 1641,\n",
       " 'will': 7479,\n",
       " 'tell': 6724,\n",
       " 'quite': 5511,\n",
       " 'long': 4191,\n",
       " 'cos': 2020,\n",
       " 'haven': 3360,\n",
       " 'finish': 2857,\n",
       " 'film': 2842,\n",
       " 'yet': 7655,\n",
       " 'worry': 7562,\n",
       " 'though': 6831,\n",
       " 'understand': 7116,\n",
       " 'important': 3629,\n",
       " 'is': 3746,\n",
       " 'be': 1284,\n",
       " 'put': 5488,\n",
       " 'place': 5217,\n",
       " 'poorly': 5295,\n",
       " 'thought': 6832,\n",
       " 'out': 5004,\n",
       " 'punishment': 5475,\n",
       " 'face': 2746,\n",
       " 'worst': 7565,\n",
       " 'thing': 6811,\n",
       " 'has': 3347,\n",
       " 'ever': 2670,\n",
       " 'happened': 3329,\n",
       " 'brb': 1498,\n",
       " 'gonna': 3182,\n",
       " 'kill': 3937,\n",
       " 'myself': 4672,\n",
       " 'pete': 5161,\n",
       " 'please': 5234,\n",
       " 'ring': 5766,\n",
       " 'meive': 4434,\n",
       " 'hardly': 3341,\n",
       " 'gotany': 3204,\n",
       " 'credit': 2071,\n",
       " 'tried': 7008,\n",
       " 'contact': 1986,\n",
       " 'reply': 5693,\n",
       " 'our': 5002,\n",
       " 'offer': 4897,\n",
       " 'video': 7254,\n",
       " 'handset': 3318,\n",
       " '750': 627,\n",
       " 'anytime': 1020,\n",
       " 'any': 1010,\n",
       " 'networks': 4755,\n",
       " 'mins': 4498,\n",
       " 'unlimited': 7142,\n",
       " 'text': 6756,\n",
       " 'camcorder': 1635,\n",
       " '08000930705': 52,\n",
       " 'dear': 2195,\n",
       " 'mood': 4583,\n",
       " 'off': 4892,\n",
       " 'cant': 1652,\n",
       " 'drive': 2459,\n",
       " 'so': 6260,\n",
       " 'brother': 1536,\n",
       " 'are': 1067,\n",
       " 'pls': 5242,\n",
       " 'open': 4955,\n",
       " 'this': 6821,\n",
       " 'weeks': 7405,\n",
       " 'savamob': 5897,\n",
       " 'member': 4440,\n",
       " 'offers': 4900,\n",
       " 'accessible': 790,\n",
       " 'just': 3876,\n",
       " '08709501522': 97,\n",
       " 'details': 2278,\n",
       " 'pobox': 5253,\n",
       " '139': 298,\n",
       " 'la3': 3990,\n",
       " '2wu': 428,\n",
       " 'only': 4944,\n",
       " '50': 543,\n",
       " 'week': 7400,\n",
       " 'mobile': 4545,\n",
       " 'voucher': 7296,\n",
       " 'holder': 3465,\n",
       " 'claim': 1828,\n",
       " 'pc': 5117,\n",
       " 'http': 3528,\n",
       " 'wtlp': 7597,\n",
       " 'co': 1877,\n",
       " 'ts': 7029,\n",
       " 'cs': 2093,\n",
       " 'apply': 1044,\n",
       " 'yeah': 7642,\n",
       " 'fact': 2748,\n",
       " 'he': 3369,\n",
       " 'asked': 1115,\n",
       " 'needed': 4733,\n",
       " 'anything': 1019,\n",
       " 'like': 4109,\n",
       " 'an': 975,\n",
       " 'hour': 3509,\n",
       " 'ago': 889,\n",
       " 'when': 7444,\n",
       " 'much': 4634,\n",
       " 'hi': 3422,\n",
       " 'darlin': 2169,\n",
       " 'im': 3614,\n",
       " 'missin': 4514,\n",
       " 'hope': 3486,\n",
       " 'having': 3364,\n",
       " 'time': 6867,\n",
       " 'give': 3149,\n",
       " 'jess': 3820,\n",
       " 'xx': 7617,\n",
       " 'do': 2377,\n",
       " 'remember': 5664,\n",
       " 'skype': 6194,\n",
       " 'later': 4029,\n",
       " 'curious': 2120,\n",
       " 'because': 1293,\n",
       " 'cuz': 2138,\n",
       " 'was': 7361,\n",
       " 'up': 7156,\n",
       " 'dont': 2414,\n",
       " 'pick': 5189,\n",
       " 'something': 6284,\n",
       " 'hrishi': 3525,\n",
       " 'mm': 4533,\n",
       " 'yes': 7652,\n",
       " 'look': 4195,\n",
       " 'am': 952,\n",
       " 'hugging': 3535,\n",
       " 'both': 1460,\n",
       " 'might': 4475,\n",
       " 'kerala': 3916,\n",
       " 'days': 2185,\n",
       " 'prepared': 5369,\n",
       " 'take': 6663,\n",
       " 'leave': 4061,\n",
       " 'once': 4936,\n",
       " 'finalise': 2848,\n",
       " 'plan': 5221,\n",
       " 'travel': 6993,\n",
       " 'during': 2499,\n",
       " 'need': 4731,\n",
       " 'urgent': 7177,\n",
       " 'works': 7557,\n",
       " 'infact': 3669,\n",
       " 'happy': 3337,\n",
       " 'new': 4762,\n",
       " 'year': 7643,\n",
       " 'where': 7448,\n",
       " 'seeing': 5965,\n",
       " 'bout': 1466,\n",
       " 'nite': 4788,\n",
       " 'wasn': 7362,\n",
       " 'fault': 2791,\n",
       " 'spouse': 6386,\n",
       " 'pmt': 5250,\n",
       " 'sumthin': 6573,\n",
       " '4give': 526,\n",
       " 'shldxxxx': 6085,\n",
       " 'care': 1662,\n",
       " 'cherthala': 1776,\n",
       " 'case': 1680,\n",
       " 'coming': 1920,\n",
       " 'cochin': 1884,\n",
       " 'bfore': 1356,\n",
       " 'start': 6422,\n",
       " 'shall': 6043,\n",
       " 'also': 945,\n",
       " 'accordingly': 799,\n",
       " 'which': 7452,\n",
       " 'day': 2184,\n",
       " 'tmorow': 6891,\n",
       " 'engaged': 2599,\n",
       " 'ans': 999,\n",
       " 'its': 3765,\n",
       " 'holiday': 3468,\n",
       " 'wan': 7342,\n",
       " 'then': 6792,\n",
       " 'din': 2327,\n",
       " 'stripes': 6497,\n",
       " 'skirt': 6192,\n",
       " 'lt': 4247,\n",
       " 'decimal': 2206,\n",
       " 'gt': 3262,\n",
       " 'common': 1926,\n",
       " 'here': 3413,\n",
       " 'better': 1348,\n",
       " 'buy': 1592,\n",
       " 'china': 1797,\n",
       " 'asia': 1110,\n",
       " 'find': 2852,\n",
       " 'less': 4079,\n",
       " 'expensive': 2720,\n",
       " 'holla': 3469,\n",
       " 'friday': 2998,\n",
       " 'pongal': 5288,\n",
       " 'get': 3120,\n",
       " 'news': 4767,\n",
       " 'work': 7552,\n",
       " 'one': 4938,\n",
       " 'partnership': 5084,\n",
       " 'lead': 4052,\n",
       " 'oh': 4910,\n",
       " 'wasted': 7364,\n",
       " 'den': 2245,\n",
       " 'muz': 4664,\n",
       " 'chiong': 1802,\n",
       " 'sat': 5889,\n",
       " 'sun': 6574,\n",
       " 'liao': 4090,\n",
       " 'plz': 5248,\n",
       " 'checking': 1761,\n",
       " 'miss': 4512,\n",
       " 'jeremiah': 3815,\n",
       " 'great': 3239,\n",
       " 'month': 4579,\n",
       " 'paying': 5112,\n",
       " 'attention': 1149,\n",
       " 'natalja': 4706,\n",
       " '25': 372,\n",
       " 'inviting': 3723,\n",
       " 'her': 3412,\n",
       " 'friend': 3001,\n",
       " '440': 502,\n",
       " 'see': 5963,\n",
       " 'sms': 6244,\n",
       " 'ac': 783,\n",
       " 'nat27081980': 4703,\n",
       " 'stop': 6467,\n",
       " 'frnd': 3010,\n",
       " '62468': 588,\n",
       " 'deal': 2191,\n",
       " 'farm': 2779,\n",
       " 'tour': 6966,\n",
       " '9am': 748,\n",
       " '5pm': 573,\n",
       " '95': 741,\n",
       " 'pax': 5108,\n",
       " 'deposit': 2257,\n",
       " '16': 321,\n",
       " 'may': 4398,\n",
       " 'house': 3511,\n",
       " '8am': 725,\n",
       " 'gonnamissu': 3183,\n",
       " 'would': 7572,\n",
       " 'say': 5903,\n",
       " 'il': 3608,\n",
       " 'postcard': 5317,\n",
       " 'buttheres': 1590,\n",
       " 'aboutas': 772,\n",
       " 'merememberin': 4453,\n",
       " 'asthere': 1128,\n",
       " 'ofsi': 4906,\n",
       " 'breakin': 1505,\n",
       " 'his': 3437,\n",
       " 'contract': 1994,\n",
       " 'yaxx': 7638,\n",
       " 'does': 2387,\n",
       " 'applebees': 1041,\n",
       " 'fucking': 3029,\n",
       " 'anyone': 1014,\n",
       " 'spare': 6334,\n",
       " 'top': 6945,\n",
       " 'head': 3370,\n",
       " 'whats': 7439,\n",
       " 'morning': 4590,\n",
       " 'world': 7558,\n",
       " 'suffers': 6556,\n",
       " 'lot': 4211,\n",
       " 'violence': 7267,\n",
       " 'bad': 1212,\n",
       " 'people': 5132,\n",
       " 'silence': 6144,\n",
       " 'gud': 3266,\n",
       " 'check': 1757,\n",
       " 'errors': 2640,\n",
       " 'difficulties': 2319,\n",
       " 'correction': 2016,\n",
       " 'phone': 5177,\n",
       " 'wanna': 7345,\n",
       " 'chat': 1748,\n",
       " 'set': 6015,\n",
       " 'meet': 4426,\n",
       " '09096102316': 252,\n",
       " 'cum': 2113,\n",
       " '2moro': 402,\n",
       " 'jane': 3791,\n",
       " 'calls': 1631,\n",
       " 'minmoremobsemspobox45po139wa': 4495,\n",
       " 'best': 1342,\n",
       " 'driving': 2462,\n",
       " 'tmr': 6893,\n",
       " 'enjoyed': 2606,\n",
       " 'content': 1990,\n",
       " '61610': 586,\n",
       " 'unsubscribe': 7151,\n",
       " 'help': 3402,\n",
       " '08712400602450p': 106,\n",
       " 'provided': 5453,\n",
       " 'tones2you': 6931,\n",
       " 'tuition': 7042,\n",
       " '330': 452,\n",
       " 'hm': 3449,\n",
       " '1120': 274,\n",
       " '1205': 283,\n",
       " 'mind': 4487,\n",
       " 'repeat': 5685,\n",
       " 'word': 7550,\n",
       " 'ok': 4914,\n",
       " 'eh': 2554,\n",
       " 'ger': 3118,\n",
       " 'toking': 6913,\n",
       " 'abt': 778,\n",
       " 'syd': 6641,\n",
       " 'haha': 3296,\n",
       " 'birthday': 1379,\n",
       " 'did': 2303,\n",
       " 'die': 2308,\n",
       " 'stay': 6434,\n",
       " 'planned': 5224,\n",
       " 'join': 3839,\n",
       " 'company': 1930,\n",
       " 'jan': 3789,\n",
       " 'happen': 3327,\n",
       " 'after': 872,\n",
       " 'interested': 3706,\n",
       " 'some': 6274,\n",
       " 'business': 1585,\n",
       " 'thesmszone': 6803,\n",
       " 'com': 1910,\n",
       " 'lets': 4084,\n",
       " 'free': 2981,\n",
       " 'anonymous': 996,\n",
       " 'masked': 4371,\n",
       " 'messages': 4459,\n",
       " 'sending': 5992,\n",
       " 'message': 4457,\n",
       " 'potential': 5324,\n",
       " 'abuse': 781,\n",
       " 'okie': 4918,\n",
       " 'done': 2412,\n",
       " 'next': 4771,\n",
       " 'space': 6328,\n",
       " 'gives': 3151,\n",
       " 'everything': 2680,\n",
       " 'furniture': 3049,\n",
       " 'yours': 7675,\n",
       " 'around': 1088,\n",
       " 'move': 4605,\n",
       " 'lock': 4171,\n",
       " 'locks': 4172,\n",
       " 'key': 3920,\n",
       " 'jenne': 3812,\n",
       " 'dled': 2372,\n",
       " '3d': 465,\n",
       " 'very': 7243,\n",
       " 'imp': 3626,\n",
       " 'sathya': 5892,\n",
       " 'till': 6865,\n",
       " 'dint': 2332,\n",
       " 'even': 2665,\n",
       " 'single': 6162,\n",
       " 'saw': 5902,\n",
       " 'situation': 6177,\n",
       " 'kallis': 3892,\n",
       " 'wont': 7543,\n",
       " 'first': 2869,\n",
       " 'two': 7062,\n",
       " 'odi': 4889,\n",
       " 'dice': 2300,\n",
       " 'art': 1099,\n",
       " 'class': 1837,\n",
       " 'thru': 6846,\n",
       " 'thanks': 6772,\n",
       " 'idea': 3587,\n",
       " 'should': 6105,\n",
       " 'tomorrow': 6926,\n",
       " 'way': 7377,\n",
       " 'true': 7020,\n",
       " 'passable': 5089,\n",
       " 'high': 3425,\n",
       " 'score': 5923,\n",
       " 'phd': 5170,\n",
       " '5years': 579,\n",
       " 'salary': 5862,\n",
       " 'makes': 4326,\n",
       " 'life': 4099,\n",
       " 'easier': 2516,\n",
       " 'babe': 1201,\n",
       " 'figure': 2834,\n",
       " 'years': 7644,\n",
       " 'de': 2189,\n",
       " 'asking': 1117,\n",
       " 'before': 1310,\n",
       " 'yetunde': 7657,\n",
       " 'said': 5858,\n",
       " 'she': 6056,\n",
       " 'wanted': 7349,\n",
       " 'surprise': 6604,\n",
       " 'didnt': 2306,\n",
       " 'money': 4572,\n",
       " 'returned': 5740,\n",
       " 'mid': 4471,\n",
       " 'january': 3793,\n",
       " 'return': 5739,\n",
       " 'period': 5144,\n",
       " 'ended': 2591,\n",
       " 'bored': 1452,\n",
       " 'depressed': 2259,\n",
       " 'sittin': 6175,\n",
       " 'waitin': 7325,\n",
       " 'wind': 7484,\n",
       " 'drops': 2467,\n",
       " 'scary': 5914,\n",
       " 'marriage': 4364,\n",
       " 'function': 3040,\n",
       " 'print': 5399,\n",
       " 'outs': 5012,\n",
       " 'da': 2144,\n",
       " 'detroit': 2281,\n",
       " 'snow': 6257,\n",
       " 'enjoy': 2605,\n",
       " 'eerie': 2544,\n",
       " 'nokia': 4802,\n",
       " 'tones': 6929,\n",
       " '4u': 538,\n",
       " 'rply': 5814,\n",
       " 'tone': 6928,\n",
       " 'title': 6880,\n",
       " '8007': 644,\n",
       " 'dracula': 2442,\n",
       " 'titles': 6881,\n",
       " 'ghost': 3132,\n",
       " 'addamsfa': 826,\n",
       " 'munsters': 4649,\n",
       " 'exorcist': 2715,\n",
       " 'twilight': 7059,\n",
       " 'getzed': 3129,\n",
       " 'bus': 1581,\n",
       " 'mate': 4380,\n",
       " 'hows': 3519,\n",
       " 'honey': 3477,\n",
       " 'ave': 1174,\n",
       " 'gimmi': 3141,\n",
       " 'goss': 3200,\n",
       " 'special': 6340,\n",
       " 'stock': 6461,\n",
       " 'talking': 6675,\n",
       " 'too': 6937,\n",
       " 'baby': 1204,\n",
       " 'promise': 5435,\n",
       " 'treat': 7000,\n",
       " 'well': 7416,\n",
       " 'actually': 822,\n",
       " 'could': 2032,\n",
       " 'yourself': 7676,\n",
       " 'yor': 7665,\n",
       " 'own': 5028,\n",
       " 've': 7234,\n",
       " 'more': 4586,\n",
       " 'than': 6769,\n",
       " 'man': 4334,\n",
       " 'pay': 5109,\n",
       " 'rent': 5679,\n",
       " 'fill': 2838,\n",
       " 'gas': 3083,\n",
       " 'tank': 6681,\n",
       " 'stressed': 6490,\n",
       " 'didn': 2305,\n",
       " 'thanksgiving': 6774,\n",
       " 'them': 6789,\n",
       " 'nothing': 4829,\n",
       " 'another': 998,\n",
       " 'smile': 6234,\n",
       " 'sunny': 6579,\n",
       " 'rays': 5559,\n",
       " 'leaves': 4062,\n",
       " 'worries': 7561,\n",
       " 'blue': 1421,\n",
       " 'bay': 1266,\n",
       " 'mrng': 4614,\n",
       " 'kate': 3901,\n",
       " 'evening': 2666,\n",
       " 'bit': 1381,\n",
       " 'bloody': 1417,\n",
       " 'babyjontet': 1205,\n",
       " 'xxx': 7620,\n",
       " 'track': 6972,\n",
       " 'lighters': 4106,\n",
       " 'theatre': 6784,\n",
       " 'wherever': 7450,\n",
       " 'jus': 3875,\n",
       " 'tot': 6959,\n",
       " 'dun': 2493,\n",
       " 'sch': 5916,\n",
       " 'today': 6905,\n",
       " 'lyricalladie': 4273,\n",
       " '21': 360,\n",
       " '910': 733,\n",
       " 'hmmross': 3454,\n",
       " 'officially': 4903,\n",
       " 'philosophical': 5173,\n",
       " 'hole': 3467,\n",
       " 'ready': 5578,\n",
       " 'saved': 5899,\n",
       " 'pure': 5481,\n",
       " 'hearted': 3384,\n",
       " 'wonderful': 7540,\n",
       " 'enemies': 2595,\n",
       " 'feel': 2805,\n",
       " 'guilty': 3275,\n",
       " 'enemy': 2596,\n",
       " 'catch': 1690,\n",
       " 'goodmorning': 3189,\n",
       " 'smiley': 6237,\n",
       " 'sunday': 6576,\n",
       " 'girls': 3146,\n",
       " 'office': 4901,\n",
       " 'wonder': 7539,\n",
       " 'smiling': 6238,\n",
       " 'sore': 6306,\n",
       " 'cinema': 1823,\n",
       " 'pass': 5088,\n",
       " '09061209465': 196,\n",
       " 'suprman': 6596,\n",
       " 'matrix3': 4387,\n",
       " 'starwars3': 6428,\n",
       " 'etc': 2656,\n",
       " 'bx420': 1601,\n",
       " 'ip4': 3730,\n",
       " '5we': 576,\n",
       " '150pm': 311,\n",
       " 'ya': 7628,\n",
       " 'sapna': 5881,\n",
       " 'aunty': 1164,\n",
       " 'manege': 4344,\n",
       " 'hogidhe': 3459,\n",
       " 'chinnu': 1801,\n",
       " 'full': 3037,\n",
       " 'weak': 7381,\n",
       " 'swalpa': 6616,\n",
       " 'black': 1389,\n",
       " 'agidhane': 887,\n",
       " 'stuff': 6514,\n",
       " 'sell': 5982,\n",
       " 'probably': 5413,\n",
       " 'things': 6812,\n",
       " 'due': 2488,\n",
       " 'several': 6023,\n",
       " 'outstanding': 5015,\n",
       " 'invoices': 3725,\n",
       " 'three': 6838,\n",
       " 'months': 4582,\n",
       " 'sends': 5993,\n",
       " 'greetings': 3246,\n",
       " 'information': 3675,\n",
       " 'orange': 4978,\n",
       " 'user': 7194,\n",
       " '0789xxxxxxx': 43,\n",
       " 'lucky': 4253,\n",
       " '2find': 393,\n",
       " 'log': 4175,\n",
       " 'onto': 4947,\n",
       " 'urawinner': 7174,\n",
       " 'fantastic': 2776,\n",
       " 'awaiting': 1183,\n",
       " 'ron': 5797,\n",
       " 'fri': 2997,\n",
       " 'ding': 2329,\n",
       " 'tai': 6659,\n",
       " 'feng': 2816,\n",
       " 'make': 4325,\n",
       " 'reservations': 5707,\n",
       " 'thank': 6771,\n",
       " 'been': 1305,\n",
       " 'miles': 4480,\n",
       " 'smiles': 6236,\n",
       " 'made': 4302,\n",
       " 'frm': 3008,\n",
       " 'same': 5873,\n",
       " 'letters': 4086,\n",
       " 'difference': 2315,\n",
       " 'keeps': 3910,\n",
       " 'away': 1187,\n",
       " 'keep': 3908,\n",
       " 'nyt': 4868,\n",
       " 'someone': 6278,\n",
       " 'conacted': 1951,\n",
       " 'dating': 2179,\n",
       " 'service': 6011,\n",
       " 'entered': 2613,\n",
       " 'they': 6806,\n",
       " 'fancy': 2773,\n",
       " 'who': 7457,\n",
       " 'landline': 4006,\n",
       " '09111030116': 258,\n",
       " 'pobox12n146tf15': 5255,\n",
       " 'directly': 2336,\n",
       " 'behind': 1320,\n",
       " 'rows': 5810,\n",
       " 'workin': 7554,\n",
       " 'job': 3832,\n",
       " 'likely': 4111,\n",
       " 'called': 1624,\n",
       " 'mittelschmertz': 4525,\n",
       " 'google': 3196,\n",
       " 'paracetamol': 5064,\n",
       " 'pain': 5046,\n",
       " 'stil': 6458,\n",
       " 'fucked': 3027,\n",
       " 'went': 7422,\n",
       " 'tobed': 6902,\n",
       " '430': 499,\n",
       " '630': 590,\n",
       " 'castor': 1687,\n",
       " 'timin': 6870,\n",
       " 'wat': 7365,\n",
       " 'lesson': 4081,\n",
       " 'until': 7154,\n",
       " 'mob': 4543,\n",
       " 'package': 5038,\n",
       " 'min': 4485,\n",
       " 'term': 6741,\n",
       " '54': 563,\n",
       " 'resubmit': 5732,\n",
       " 'request': 5699,\n",
       " 'expiry': 2726,\n",
       " 'themob': 6791,\n",
       " 'info': 3673,\n",
       " 'right': 5761,\n",
       " 'trip': 7009,\n",
       " 'watch': 7366,\n",
       " 'must': 4658,\n",
       " 'decide': 2203,\n",
       " 'easter': 2520,\n",
       " 'youphone': 7670,\n",
       " 'athome': 1138,\n",
       " 'youwanna': 7678,\n",
       " 'whos': 7462,\n",
       " 'mail': 4318,\n",
       " 'panren': 5057,\n",
       " 'paru': 5086,\n",
       " 'waqt': 7353,\n",
       " 'se': 5943,\n",
       " 'pehle': 5125,\n",
       " 'naseeb': 4700,\n",
       " 'zyada': 7702,\n",
       " 'kisi': 3949,\n",
       " 'ko': 3966,\n",
       " 'kuch': 3978,\n",
       " 'nahi': 4683,\n",
       " 'milta': 4484,\n",
       " 'zindgi': 7698,\n",
       " 'wo': 7529,\n",
       " 'jo': 3830,\n",
       " 'hum': 3541,\n",
       " 'sochte': 6262,\n",
       " 'hai': 3298,\n",
       " 'ham': 3310,\n",
       " 'jeetey': 3808,\n",
       " 'reason': 5592,\n",
       " 'team': 6706,\n",
       " 'budget': 1560,\n",
       " 'available': 1171,\n",
       " 'unsold': 7149,\n",
       " 'players': 5230,\n",
       " 'base': 1247,\n",
       " 'rate': 5551,\n",
       " 'whatever': 7438,\n",
       " 'rules': 5830,\n",
       " 'talk': 6671,\n",
       " 'earlier': 2510,\n",
       " 'happening': 3331,\n",
       " 'showing': 6115,\n",
       " 'responsibility': 5724,\n",
       " 'bend': 1335,\n",
       " 'rule': 5829,\n",
       " 'tired': 6875,\n",
       " 'thia': 6809,\n",
       " 'argument': 1075,\n",
       " 'movie': 4608,\n",
       " 'doesnt': 2390,\n",
       " 'inlude': 3680,\n",
       " 'previews': 5389,\n",
       " 'getting': 3128,\n",
       " 'congratulations': 1973,\n",
       " 'awarded': 1186,\n",
       " '500': 544,\n",
       " 'cd': 1702,\n",
       " 'vouchers': 7297,\n",
       " '125gift': 290,\n",
       " 'guaranteed': 3264,\n",
       " 'entry': 2622,\n",
       " '100': 261,\n",
       " 'wkly': 7522,\n",
       " 'draw': 2447,\n",
       " '87066': 697,\n",
       " 'tncs': 6898,\n",
       " 'ldew': 4048,\n",
       " 'com1win150ppmx3age16': 1911,\n",
       " '88066': 708,\n",
       " 'lost': 4210,\n",
       " '12': 282,\n",
       " 'captain': 1655,\n",
       " 'aiyah': 906,\n",
       " 'tv': 7053,\n",
       " 'forgot': 2948,\n",
       " 'ask': 1113,\n",
       " 'pa': 5035,\n",
       " 'point': 5272,\n",
       " 'hangin': 3321,\n",
       " 'mr': 4612,\n",
       " 'makin': 4327,\n",
       " 'shoot': 6090,\n",
       " 'big': 1361,\n",
       " 'loads': 4163,\n",
       " 'close': 1855,\n",
       " 'eyes': 2741,\n",
       " 'vava': 7231,\n",
       " 'playing': 5232,\n",
       " 'umma': 7098,\n",
       " 'many': 4349,\n",
       " 'times': 6868,\n",
       " 'told': 6915,\n",
       " 'stage': 6406,\n",
       " 'laugh': 4032,\n",
       " 'listen': 4141,\n",
       " 'aha': 892,\n",
       " 'train': 6977,\n",
       " 'northampton': 4821,\n",
       " 'afraid': 868,\n",
       " 'staying': 6437,\n",
       " 'skyving': 6196,\n",
       " 'ho': 3457,\n",
       " 'wednesday': 7396,\n",
       " 'comedy': 1916,\n",
       " 'club': 1864,\n",
       " 'feathery': 2798,\n",
       " 'bowa': 1467,\n",
       " 'guys': 3282,\n",
       " 'hello': 3400,\n",
       " 'doing': 2401,\n",
       " 'interview': 3711,\n",
       " 'missing': 4515,\n",
       " 'txting': 7069,\n",
       " 'tiwary': 6882,\n",
       " 'rcb': 5560,\n",
       " 'battle': 1265,\n",
       " 'between': 1351,\n",
       " 'bang': 1227,\n",
       " 'kochi': 3967,\n",
       " 'road': 5780,\n",
       " 'him': 3430,\n",
       " 'thats': 6781,\n",
       " 'kind': 3942,\n",
       " 'get4an18th': 3121,\n",
       " 'loves': 4233,\n",
       " 'crickiting': 2079,\n",
       " 'acknowledgement': 806,\n",
       " 'astoundingly': 1131,\n",
       " 'tactless': 6656,\n",
       " 'generally': 3106,\n",
       " 'faggy': 2752,\n",
       " 'demand': 2244,\n",
       " 'blood': 1416,\n",
       " 'oath': 4874,\n",
       " 'fo': 2916,\n",
       " 'always': 950,\n",
       " 'putting': 5491,\n",
       " 'pictures': 5196,\n",
       " 'ass': 1121,\n",
       " 'facebook': 2747,\n",
       " 'met': 4464,\n",
       " 'picture': 5195,\n",
       " 'hurt': 3555,\n",
       " 'violated': 7266,\n",
       " 'model': 4557,\n",
       " 'num': 4850,\n",
       " 'as': 1104,\n",
       " 'soon': 6299,\n",
       " 'ha': 3289,\n",
       " 'wouldn': 7574,\n",
       " 'read': 5574,\n",
       " 'into': 3714,\n",
       " 'seemed': 5969,\n",
       " 'judgemental': 3864,\n",
       " 'save': 5898,\n",
       " 'fridays': 2999,\n",
       " 'pub': 5466,\n",
       " 'early': 2512,\n",
       " 'hor': 3493,\n",
       " 'bloo': 1415,\n",
       " 'bowl': 1468,\n",
       " 'bring': 1520,\n",
       " 'returns': 5742,\n",
       " 'wish': 7501,\n",
       " 'official': 4902,\n",
       " 'england': 2603,\n",
       " 'poly': 5281,\n",
       " 'ringtone': 5769,\n",
       " 'colour': 1907,\n",
       " 'flag': 2881,\n",
       " 'yer': 7651,\n",
       " 'tonights': 6933,\n",
       " 'game': 3068,\n",
       " '84199': 682,\n",
       " 'optout': 4974,\n",
       " 'eng': 2598,\n",
       " 'box39822': 1477,\n",
       " 'w111wx': 7303,\n",
       " 'maybe': 4400,\n",
       " '7ish': 636,\n",
       " 'sir': 6167,\n",
       " 'weekend': 7402,\n",
       " 'able': 769,\n",
       " 'raise': 5528,\n",
       " 'dad': 2148,\n",
       " 'however': 3518,\n",
       " 'rest': 5725,\n",
       " 'feb': 2800,\n",
       " 'amount': 967,\n",
       " 'short': 6096,\n",
       " 'hoping': 3492,\n",
       " 'abiola': 767,\n",
       " 'hitter': 3443,\n",
       " 'anyway': 1021,\n",
       " 'although': 948,\n",
       " 'dat': 2174,\n",
       " 'baig': 1217,\n",
       " 'watches': 7368,\n",
       " 'gave': 3089,\n",
       " 'fr': 2968,\n",
       " 'thanx': 6776,\n",
       " 'touched': 6964,\n",
       " 'brum': 1545,\n",
       " 'keeping': 3909,\n",
       " 'copy': 2011,\n",
       " 'speak': 6338,\n",
       " 'sunshine': 6582,\n",
       " 'quiz': 5513,\n",
       " 'sony': 6296,\n",
       " 'dvd': 2503,\n",
       " 'player': 5229,\n",
       " 'country': 2038,\n",
       " 'algarve': 924,\n",
       " 'ansr': 1000,\n",
       " '82277': 662,\n",
       " 'sp': 6327,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.vocabulary_  #worsd index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_np = x_train_transformed.toarray()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 980, 1135, 1557, 2611, 2854, 3202, 4023, 4059, 4594, 4780, 4890,\n",
       "        6778, 6782, 6899]),)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x_train_np[0] != 0) # words in message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultinomialNB</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_transformed, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = v.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       982\n",
      "           1       0.97      0.93      0.95       133\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(X_test_cv)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails = [\n",
    "    'Hey mohan, can we get together to watch football game tomorrow?',\n",
    "    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n",
    "]\n",
    "emails_count = v.transform(emails)\n",
    "model.predict(emails_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "len(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "are\n",
      "two\n",
      "about\n",
      "an\n",
      "is\n",
      "a\n",
      "former\n",
      "who\n",
      "the\n",
      "to\n",
      "in\n",
      "the\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "most\n",
      "and\n",
      "most\n",
      "Here\n",
      "are\n",
      "some\n",
      "other\n",
      "about\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('''Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
    "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup. \n",
    "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen. \n",
    "Here are some other facts about MS Dhoni''')\n",
    "for i in doc :\n",
    "    if i.is_stop:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[short, sentences, Mahendra, Singh, Dhoni, (, MS, Dhoni, ), ,, Indian, cricketer, :, \n",
      ", MS, Dhoni, Indian, cricketer, captained, Indian, national, team, victory, 2011, Cricket, World, Cup, ., \n",
      ", Dhoni, considered, successful, Indian, captains, prolific, wicket, -, keeper, batsmen, ., \n",
      ", facts, MS, Dhoni]\n"
     ]
    }
   ],
   "source": [
    "non = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('''Here are two short sentences about Mahendra Singh Dhoni (MS Dhoni), an Indian cricketer:\n",
    "MS Dhoni is a former Indian cricketer who captained the Indian national team to victory in the 2011 Cricket World Cup. \n",
    "Dhoni is considered one of the most successful Indian captains and most prolific wicket-keeper batsmen. \n",
    "Here are some other facts about MS Dhoni''')\n",
    "for i in doc :\n",
    "    if not  i.is_stop:\n",
    "        non.append(i)\n",
    "print(non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model outside the function\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def stopfunction(text):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "    \n",
    "    doc = nlp(text)  # Process the text with SpaCy\n",
    "    non_stop_words = [token.text for token in doc if not token.is_stop]\n",
    "    \n",
    "    return non_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'test', 'function', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence to test the function.\"\n",
    "result = stopfunction(text)\n",
    "print(result)  # Output: ['example', 'sentence', 'test', 'function']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thior is': 3, 'is living': 1, 'living in': 2, 'in chennai': 0}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n-gram\n",
    "v = CountVectorizer(ngram_range=(2,2))\n",
    "v.fit([\" thior is living in chennai\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thior': 9,\n",
       " 'is': 3,\n",
       " 'living': 6,\n",
       " 'in': 1,\n",
       " 'chennai': 0,\n",
       " 'thior is': 10,\n",
       " 'is living': 4,\n",
       " 'living in': 7,\n",
       " 'in chennai': 2,\n",
       " 'thior is living': 11,\n",
       " 'is living in': 5,\n",
       " 'living in chennai': 8}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n-gram\n",
    "v = CountVectorizer(ngram_range=(1,3))\n",
    "v.fit([\" thior is living in chennai\"])\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)  # Assuming 'nlp' is a valid Spacy model\n",
    "    filtered = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:  # Exclude stop words and punctuation\n",
    "            filtered.append(token.lemma_)  # Append the lemmatized version\n",
    "    return ' '.join(filtered)  # Join tokens with spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ananth walk dhoni lewis start ai company\n"
     ]
    }
   ],
   "source": [
    "text = \"Ananth Walking with dhoni and lewis and started a ai company together\"\n",
    "processed_text = preprocess(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick brown fox jump lazy dog', 'Natural Language processing exciting field AI', 'Python versatile programming language', 'spacy make text preprocesse easy', 'list sentence demonstration purpose']\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Natural Language Processing is an exciting field of AI.\",\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"Spacy makes text preprocessing much easier!\",\n",
    "    \"This is a list of sentences for demonstration purposes.\"\n",
    "]\n",
    "# Example: Preprocess each sentence\n",
    "preprocessed_texts = [preprocess(sentence) for sentence in texts]\n",
    "print(preprocessed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick brown': 15,\n",
       " 'brown fox': 0,\n",
       " 'fox jump': 4,\n",
       " 'jump lazy': 5,\n",
       " 'lazy dog': 7,\n",
       " 'natural language': 10,\n",
       " 'language processing': 6,\n",
       " 'processing exciting': 12,\n",
       " 'exciting field': 2,\n",
       " 'field ai': 3,\n",
       " 'python versatile': 14,\n",
       " 'versatile programming': 19,\n",
       " 'programming language': 13,\n",
       " 'spacy make': 17,\n",
       " 'make text': 9,\n",
       " 'text preprocesse': 18,\n",
       " 'preprocesse easy': 11,\n",
       " 'list sentence': 8,\n",
       " 'sentence demonstration': 16,\n",
       " 'demonstration purpose': 1}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(ngram_range=(2,2))\n",
    "v.fit(preprocessed_texts)\n",
    "v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "result = v.transform([\"ananth not a lazy dog\"]).toarray()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('news_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Watching Schrödinger's Cat Die University of C...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WATCH: Freaky Vortex Opens Up In Flooded Lake</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entrepreneurs Today Don't Need a Big Budget to...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These Roads Could Recharge Your Electric Car A...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Civilian 'Guard' Fires Gun While 'Protecting' ...</td>\n",
       "      <td>CRIME</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0  Watching Schrödinger's Cat Die University of C...   SCIENCE\n",
       "1     WATCH: Freaky Vortex Opens Up In Flooded Lake    SCIENCE\n",
       "2  Entrepreneurs Today Don't Need a Big Budget to...  BUSINESS\n",
       "3  These Roads Could Recharge Your Electric Car A...  BUSINESS\n",
       "4  Civilian 'Guard' Fires Gun While 'Protecting' ...     CRIME"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "BUSINESS    4254\n",
       "SPORTS      4167\n",
       "CRIME       2893\n",
       "SCIENCE     1381\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sample = 1381\n",
    "\n",
    "df_business = data[data.category==\"BUSINESS\"].sample(min_sample,random_state=2022)\n",
    "\n",
    "df_SPORTS = data[data.category==\"SPORTS\"].sample(min_sample,random_state=2022)\n",
    "df_CRIME = data[data.category==\"CRIME\"].sample(min_sample,random_state=2022)\n",
    "df_SCIENCE = data[data.category==\"SCIENCE\"].sample(min_sample,random_state=2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data= pd.concat([df_business,df_SPORTS,df_CRIME,df_SCIENCE ],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11967</th>\n",
       "      <td>GCC Business Leaders Remain Confident in the F...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>From the Other Side; an Honest Review from Emp...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Mike McDerment, CEO of FreshBooks, Talks About...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>How to Market Your Business While Traveling th...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>How to Leverage Intuition in Decision-making I...</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>Aquarium To Monitor Animals' Behavior Changes ...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>How Google Glass Could Save Lives In The Hospi...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>Honda's Gravity Modification Research For us A...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>EVERYONE Loves Alternative Facts THE POWER OF ...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>From Cooking to Conservation: Women Take Actio...</td>\n",
       "      <td>SCIENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "11967  GCC Business Leaders Remain Confident in the F...  BUSINESS\n",
       "2912   From the Other Side; an Honest Review from Emp...  BUSINESS\n",
       "3408   Mike McDerment, CEO of FreshBooks, Talks About...  BUSINESS\n",
       "502    How to Market Your Business While Traveling th...  BUSINESS\n",
       "5279   How to Leverage Intuition in Decision-making I...  BUSINESS\n",
       "...                                                  ...       ...\n",
       "2178   Aquarium To Monitor Animals' Behavior Changes ...   SCIENCE\n",
       "5682   How Google Glass Could Save Lives In The Hospi...   SCIENCE\n",
       "1643   Honda's Gravity Modification Research For us A...   SCIENCE\n",
       "11428  EVERYONE Loves Alternative Facts THE POWER OF ...   SCIENCE\n",
       "8101   From Cooking to Conservation: Women Take Actio...   SCIENCE\n",
       "\n",
       "[5524 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {\"BUSINESS\":0 , \"SPORTS\":1 ,\"CRIME\":2 , \"SCIENCE\":3} \n",
    "new_data['category_num'] = new_data.category.map(\n",
    "    {\"BUSINESS\":0 , \"SPORTS\":1 ,\"CRIME\":2 , \"SCIENCE\":3} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11967</th>\n",
       "      <td>GCC Business Leaders Remain Confident in the F...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>From the Other Side; an Honest Review from Emp...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Mike McDerment, CEO of FreshBooks, Talks About...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>How to Market Your Business While Traveling th...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>How to Leverage Intuition in Decision-making I...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>Aquarium To Monitor Animals' Behavior Changes ...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>How Google Glass Could Save Lives In The Hospi...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>Honda's Gravity Modification Research For us A...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>EVERYONE Loves Alternative Facts THE POWER OF ...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>From Cooking to Conservation: Women Take Actio...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5524 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category  \\\n",
       "11967  GCC Business Leaders Remain Confident in the F...  BUSINESS   \n",
       "2912   From the Other Side; an Honest Review from Emp...  BUSINESS   \n",
       "3408   Mike McDerment, CEO of FreshBooks, Talks About...  BUSINESS   \n",
       "502    How to Market Your Business While Traveling th...  BUSINESS   \n",
       "5279   How to Leverage Intuition in Decision-making I...  BUSINESS   \n",
       "...                                                  ...       ...   \n",
       "2178   Aquarium To Monitor Animals' Behavior Changes ...   SCIENCE   \n",
       "5682   How Google Glass Could Save Lives In The Hospi...   SCIENCE   \n",
       "1643   Honda's Gravity Modification Research For us A...   SCIENCE   \n",
       "11428  EVERYONE Loves Alternative Facts THE POWER OF ...   SCIENCE   \n",
       "8101   From Cooking to Conservation: Women Take Actio...   SCIENCE   \n",
       "\n",
       "       category_num  \n",
       "11967             0  \n",
       "2912              0  \n",
       "3408              0  \n",
       "502               0  \n",
       "5279              0  \n",
       "...             ...  \n",
       "2178              3  \n",
       "5682              3  \n",
       "1643              3  \n",
       "11428             3  \n",
       "8101              3  \n",
       "\n",
       "[5524 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#word emberdding :\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#word to vector : CBOW (Continuous Bag of Words) predict using neibour words Skip-Gram: predict context using target word\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Small dataset of sentences\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrown\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minnocent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "#word emberdding :\n",
    "#word to vector : CBOW (Continuous Bag of Words) predict using neibour words Skip-Gram: predict context using target word\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Small dataset of sentences\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\", \"child\", \"throne\", \"royal\"],\n",
    "    [\"queen\", \"woman\", \"royal\", \"crown\"],\n",
    "    [\"king\", \"man\", \"throne\", \"royal\", \"power\"],\n",
    "    [\"child\", \"toy\", \"play\", \"innocent\"]\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=3, min_count=1, sg=1)  # sg=1 for Skip-Gram\n",
    "\n",
    "# Arithmetic: King - Man + Woman\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"Result of King - Man + Woman:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/24.0 MB 14.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.8/24.0 MB 15.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.2/24.0 MB 15.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 17.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 19.9/24.0 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 17.9 MB/s eta 0:00:00\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 28.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 11.0/46.2 MB 26.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.5/46.2 MB 26.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 23.1/46.2 MB 27.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.1/46.2 MB 27.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.8/46.2 MB 26.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.8/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.3/46.2 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 24.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.2 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "langchain 0.0.284 requires langsmith<0.1.0,>=0.0.21, but you have langsmith 0.1.147 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'wordnet' from\n",
      "[nltk_data]     <https://raw.githubusercontent.com/nltk/nltk_data/gh-\n",
      "[nltk_data]     pages/packages/corpora/wordnet.zip>:   [Errno 28] No\n",
      "[nltk_data]     space left on device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the punkt tokenizer\n",
    "nltk.download('stopwords')  # Download stopwords\n",
    "nltk.download('wordnet')  # Download WordNet for lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#word emberdding :\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#word to vector : CBOW (Continuous Bag of Words) predict using neibour words Skip-Gram: predict context using target word\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Small dataset of sentences\u001b[39;00m\n\u001b[0;32m      6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwoman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrown\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      9\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minnocent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "#word emberdding :\n",
    "#word to vector : CBOW (Continuous Bag of Words) predict using neibour words Skip-Gram: predict context using target word\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Small dataset of sentences\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"man\", \"woman\", \"child\", \"throne\", \"royal\"],\n",
    "    [\"queen\", \"woman\", \"royal\", \"crown\"],\n",
    "    [\"king\", \"man\", \"throne\", \"royal\", \"power\"],\n",
    "    [\"child\", \"toy\", \"play\", \"innocent\"]\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=3, min_count=1, sg=1)  # sg=1 for Skip-Gram\n",
    "\n",
    "# Arithmetic: King - Man + Woman\n",
    "result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"Result of King - Man + Woman:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe is based on global word co-occurrence statistics \n",
    "#worvec is unidirectional approch where as  bert is uniderctional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King's vector: [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings (let's assume it's a 50-dimensional model)\n",
    "embedding_file = 'glove.6B.100d.txt'  # Change to the actual path of your GloVe file\n",
    "\n",
    "# Create a dictionary to hold the word vectors\n",
    "embeddings = {}\n",
    "\n",
    "with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "\n",
    "# Example: Get the vector for \"king\"\n",
    "king_vector = embeddings['king']\n",
    "print(\"King's vector:\", king_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Natural Language Processing is an exciting field of AI.\",\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"Spacy makes text preprocessing much easier!\",\n",
    "    \"This is a list of sentences for demonstration purposes.\"\n",
    "]\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "v.fit(corpus)\n",
    "transformed_output = v.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 29, 'quick': 25, 'brown': 2, 'fox': 9, 'jumps': 11, 'over': 19, 'lazy': 13, 'dog': 4, 'natural': 17, 'language': 12, 'processing': 21, 'is': 10, 'an': 1, 'exciting': 6, 'field': 7, 'of': 18, 'ai': 0, 'python': 24, 'versatile': 31, 'programming': 22, 'spacy': 27, 'makes': 15, 'text': 28, 'preprocessing': 20, 'much': 16, 'easier': 5, 'this': 30, 'list': 14, 'sentences': 26, 'for': 8, 'demonstration': 3, 'purposes': 23}\n"
     ]
    }
   ],
   "source": [
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "1.4054651081081644\n",
      "2.09861228866811\n",
      "1.6931471805599454\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "1.6931471805599454\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n",
      "2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "all_feature_anmes = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_anmes:\n",
    "    indx = v.vocabulary_.get(word)\n",
    "    print(v.idf_[indx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.30151134, 0.        , 0.30151134,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30151134,\n",
       "        0.        , 0.30151134, 0.        , 0.30151134, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30151134,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30151134, 0.        , 0.        , 0.        , 0.60302269,\n",
       "        0.        , 0.        ],\n",
       "       [0.35920259, 0.35920259, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35920259, 0.35920259, 0.        , 0.        ,\n",
       "        0.24056216, 0.        , 0.28980239, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35920259, 0.28980239, 0.        ,\n",
       "        0.        , 0.35920259, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_output.toarray()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Text  \\\n",
      "0      Urban Ladder Eisner Low Back Study-Office Comp...   \n",
      "1      Contrast living Wooden Decorative Box,Painted ...   \n",
      "2      IO Crest SY-PCI40010 PCI RAID Host Controller ...   \n",
      "3      ISAKAA Baby Socks from Just Born to 8 Years- P...   \n",
      "4      Indira Designer Women's Art Mysore Silk Saree ...   \n",
      "...                                                  ...   \n",
      "23995                 Marvel Physics MCQ's for MHT - CET   \n",
      "23996  Internet Download Manager | Lifetime License |...   \n",
      "23997  Sadhubela's Handcrafted Iron Degchi Handi Pot ...   \n",
      "23998  Audio-Technica AT-LP60 Automatic Belt Driven D...   \n",
      "23999  LG GH24NSB0 DVD Writer 24X SATA Internal OEM P...   \n",
      "\n",
      "                        label  \n",
      "0                   Household  \n",
      "1                   Household  \n",
      "2                 Electronics  \n",
      "3      Clothing & Accessories  \n",
      "4      Clothing & Accessories  \n",
      "...                       ...  \n",
      "23995                   Books  \n",
      "23996                   Books  \n",
      "23997               Household  \n",
      "23998             Electronics  \n",
      "23999             Electronics  \n",
      "\n",
      "[24000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Ecommerce_data.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Household                 6000\n",
       "Electronics               6000\n",
       "Clothing & Accessories    6000\n",
       "Books                     6000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "         ..\n",
       "23995   NaN\n",
       "23996   NaN\n",
       "23997   NaN\n",
       "23998   NaN\n",
       "23999   NaN\n",
       "Name: label, Length: 24000, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.map({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_num'] = df.label.map({\n",
    "    'Household':0,\n",
    "    'Electronics':2,\n",
    "    'Books':1,\n",
    "    'Clothing & Accessories':3\n",
    "\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urban Ladder Eisner Low Back Study-Office Comp...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contrast living Wooden Decorative Box,Painted ...</td>\n",
       "      <td>Household</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IO Crest SY-PCI40010 PCI RAID Host Controller ...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISAKAA Baby Socks from Just Born to 8 Years- P...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indira Designer Women's Art Mysore Silk Saree ...</td>\n",
       "      <td>Clothing &amp; Accessories</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                   label  \\\n",
       "0  Urban Ladder Eisner Low Back Study-Office Comp...               Household   \n",
       "1  Contrast living Wooden Decorative Box,Painted ...               Household   \n",
       "2  IO Crest SY-PCI40010 PCI RAID Host Controller ...             Electronics   \n",
       "3  ISAKAA Baby Socks from Just Born to 8 Years- P...  Clothing & Accessories   \n",
       "4  Indira Designer Women's Art Mysore Silk Saree ...  Clothing & Accessories   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          2  \n",
       "3          3  \n",
       "4          3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.Text, df.label_num, test_size=0.2, random_state=2022, stratify=df.label_num\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1200\n",
      "           1       0.97      0.95      0.96      1200\n",
      "           2       0.97      0.97      0.97      1200\n",
      "           3       0.97      0.98      0.97      1200\n",
      "\n",
      "    accuracy                           0.96      4800\n",
      "   macro avg       0.96      0.96      0.96      4800\n",
      "weighted avg       0.96      0.96      0.96      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the pipeline\n",
    "clf = Pipeline([\n",
    "    ('vectorizer_tfid', TfidfVectorizer()),\n",
    "    ('KNN', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.3/10.1 MB 6.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.1 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/10.1 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.1 MB 5.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.8/10.1 MB 5.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.8/10.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.9/10.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.9/10.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Using cached safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 5.2 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\safetensors\\\\__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
